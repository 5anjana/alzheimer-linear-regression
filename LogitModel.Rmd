---
title: "Logistic Regression Model"
author: "SQR"
date: "2024-08-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
alz_data <- read.csv('alzheimers_disease_data.csv')
alz_data <- alz_data[,-c(1,35)] 
```

```{r}
alz_cat_data = alz_data %>%
  mutate(Gender = ifelse(Gender == 0,"M","F")) %>%
  mutate(Ethnicity = ifelse(Ethnicity == 0, "Caucasian",
                     ifelse(Ethnicity == 1, "African American",
                     ifelse(Ethnicity == 2, "Asian", "Other")))) %>%
  mutate(EducationLevel = ifelse(EducationLevel == 0, "None",
                          ifelse(EducationLevel == 1, "High School",
                          ifelse(EducationLevel == 2, "Bachelor's", "Higher")))) %>%
  mutate(Smoking = ifelse(Smoking == 0, "No", "Yes")) %>%
  mutate(FamilyHistoryAlzheimers = ifelse(FamilyHistoryAlzheimers == 0, "No", "Yes")) %>%
  mutate(CardiovascularDisease = ifelse(CardiovascularDisease == 0, "No", "Yes")) %>%
  mutate(Diabetes = ifelse(Diabetes == 0, "No", "Yes")) %>%
  mutate(Depression = ifelse(Depression == 0, "No", "Yes")) %>%
  mutate(HeadInjury = ifelse(HeadInjury == 0, "No", "Yes")) %>%
  mutate(Hypertension = ifelse(Hypertension == 0, "No", "Yes")) %>%
  mutate(MemoryComplaints = ifelse(MemoryComplaints == 0, "No", "Yes")) %>%
  mutate(BehavioralProblems = ifelse(BehavioralProblems == 0, "No", "Yes")) %>%
  mutate(Confusion = ifelse(Confusion == 0, "No", "Yes")) %>%
  mutate(Disorientation = ifelse(Disorientation == 0, "No", "Yes")) %>%
  mutate(PersonalityChanges = ifelse(PersonalityChanges == 0, "No", "Yes")) %>%
  mutate(DifficultyCompletingTasks = ifelse(DifficultyCompletingTasks == 0, "No", "Yes")) %>%
  mutate(Forgetfulness = ifelse(Forgetfulness == 0, "No", "Yes"))
```


Reconstruct the corrplot with MMSE included this time:
```{r}
numerics <- alz_data%>%
  dplyr::select(Age, BMI, AlcoholConsumption, PhysicalActivity, DietQuality, SleepQuality, SystolicBP,
                DiastolicBP, CholesterolTotal, CholesterolLDL, CholesterolHDL, CholesterolTriglycerides,
                MMSE, FunctionalAssessment, ADL)

cor_mat = round(cor(numerics),2)
ggcorrplot::ggcorrplot(cor_mat, lab=TRUE, lab_size=2, tl.cex = 10, method = "square")+
  scale_fill_gradient2(limits = c(-0.2, 0.2), low = "blue", mid = "white", high = "red", midpoint = 0, space = "Lab", name = "Correlation")

```
No difference. 


## Construct the logit model

### Pre log transformations

First with all predictors, no variable transformations:
```{r}
mod1 <- glm(Diagnosis~., alz_cat_data, family='binomial')
summary(mod1)
```

Reduce model to only include significant predictors; here we will only include predictors that are significant at the 0.05 level:
```{r}
mod2 <- glm(Diagnosis~CholesterolLDL+MMSE+FunctionalAssessment+MemoryComplaints+ADL, alz_data, family='binomial')
summary(mod2)
```

Now using AIC to select variables:
```{r}
aic = MASS::stepAIC(mod1, direction='both', trace=F)
summary(aic)
```


### With log tranformations

```{r}
alz_log_data = mutate(alz_cat_data,
                      log_Age = log(Age), 
                      log_BMI = log(BMI),
                      log_AlcoholConsumption = log(AlcoholConsumption),
                      log_PhysicalActivity = log(PhysicalActivity),
                      log_DietQuality = log(DietQuality),
                      log_SleepQuality = log(SleepQuality),
                      log_SystolicBP = log(SystolicBP),
                      log_DiastolicBP = log(DiastolicBP),
                      log_CholesterolTotal = log(CholesterolTotal),
                      log_CholesterolLDL = log(CholesterolLDL),
                      log_CholesterolHDL = log(CholesterolHDL),
                      log_CholesterolTriglycerides = log(CholesterolTriglycerides),
                      log_MMSE = log(MMSE),
                      log_FunctionalAssessment = log(FunctionalAssessment),
                      log_ADL = log(ADL))

alz_log_data = subset(alz_log_data, select = -c(Age,BMI,AlcoholConsumption,PhysicalActivity,
                                               DietQuality,SleepQuality,SystolicBP,DiastolicBP,
                                               CholesterolTotal, CholesterolLDL,CholesterolHDL,
                                               CholesterolTriglycerides,MMSE,FunctionalAssessment,ADL))
```


Model with all log predictors:
```{r}
mod11 <- glm(Diagnosis~., alz_log_data, family='binomial')
summary(mod11)
```

Using AIC to select variables:
```{r}
aic2 = MASS::stepAIC(mod11, direction='both', trace=F)
summary(aic2)
```


## Moving forward without log transformations

The AICs for the previous models (pre log transformations) are better, so we'll go with 'aic' (defined above) as our best model for now. 
```{r}
# Define model with AIC selected predictors
aic_mod <- glm(Diagnosis~Age+Smoking+SleepQuality+HeadInjury+CholesterolLDL+
                CholesterolHDL+MMSE+FunctionalAssessment+MemoryComplaints+BehavioralProblems+ADL,
                alz_cat_data, family='binomial')
summary(aic_mod)
```


VIFs:
```{r}
car::vif(aic_mod)
```

Adding interaction effects:
```{r}
aic_mod2 <- glm(Diagnosis~(Age+Smoking+SleepQuality+HeadInjury+CholesterolLDL+CholesterolHDL
                +MMSE+FunctionalAssessment+MemoryComplaints+BehavioralProblems+ADL)^2,
                alz_cat_data, family='binomial')
summary(aic_mod2)
```

Let's use StepAIC again to refine this new set of predictors:
```{r}
selected_model = MASS::stepAIC(aic_mod2, direction='both', trace=F)
summary(selected_model)
```
This is the lowest AIC we've achieved thus far, so we'll move forward with this model. 


## Residual plot
```{r}
# I'm not sure if residual plots and QQ plots are required for the logit model, but I made them just in case
## I don't think the same principles apply when looking at logistic residuals, we could ask him about what's required to show that we've met model assumptions for the logistic model
alz_cat_data_pred <- alz_cat_data%>%
  mutate(pred = selected_model$fitted.values, resid = selected_model$residuals)

ggplot(alz_cat_data_pred , aes(x=pred, y=resid))+
  geom_point()+
  geom_hline(yintercept =0, color="red")
```

## QQ plot
```{r}
ggplot(alz_cat_data_pred, aes(sample=resid)) +
  stat_qq() +
  stat_qq_line(color="red")
```

## RMSE, R squared, MAE
```{r}
caret::RMSE(alz_cat_data_pred$pred, alz_cat_data_pred$Diagnosis)
```
```{r}
caret::R2(alz_cat_data_pred$pred, alz_cat_data_pred$Diagnosis)
```
```{r}
caret::MAE(alz_cat_data_pred$pred, alz_cat_data_pred$Diagnosis)
```


## Calculate accuracy

I'm not sure how else to interpret the model, so I'm going to encode all predictions above 0.5 as 1 and all predictions below 0.5 as 0. Then we can calculate the model's accuracy. 
```{r}
alz_cat_data_pred <- alz_cat_data_pred%>%
  mutate(predicted_diagnosis = ifelse(pred >= 0.5, 1, 0))

# Calculate accuracy
table(alz_cat_data_pred$Diagnosis, alz_cat_data_pred$predicted_diagnosis)
```
Accuracy = (1297+588) / (1297+588+172+92) = 87.7% based on a threshold of 0.5. 

As a comparison, the SVM classification model yielded an accuracy of 93.8%. SVMs aim to identify the hyperplane that best separates the data into different classes. This approach works well with high dimensional data where the separation between classes is not obvious. 


## Removing outliers/influential points/leverage points

## Cross validation



