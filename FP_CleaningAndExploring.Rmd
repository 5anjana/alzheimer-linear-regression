---
title: "Final Project: Data Cleaning and Exploration"
author: "Lucy Shichman"
date: "2024-07-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# STAT 6021 Final Project: Predicting Alzheimer's Diagnosis

## Data Cleaning
### source: <https://www.kaggle.com/datasets/rabieelkharoua/alzheimers-disease-dataset?resource=download>
```{r, echo=FALSE}
library(tidyverse)
library(ggcorrplot)
library(MASS)
library(broom)
```

```{r}
alz_data = read.csv("/Users/lucyshichman/Documents/MSDS/STAT6021/alzheimer-linear-regression/alzheimers_disease_data.csv")
```

Dropping column with undisclosed classified info:
```{r}
alz_data = alz_data[,-35]
```

Creating version of data w/ numerically encoding variables converted to categorical:
```{r}
alz_cat_data = alz_data %>%
  mutate(Gender = ifelse(Gender == 0,"M","F")) %>%
  mutate(Ethnicity = ifelse(Ethnicity == 0, "Caucasian",
                     ifelse(Ethnicity == 1, "African American",
                     ifelse(Ethnicity == 2, "Asian", "Other")))) %>%
  mutate(EducationLevel = ifelse(EducationLevel == 0, "None",
                          ifelse(EducationLevel == 1, "High School",
                          ifelse(EducationLevel == 2, "Bachelor's", "Higher")))) %>%
  mutate(Smoking = ifelse(Smoking == 0, "No", "Yes")) %>%
  mutate(FamilyHistoryAlzheimers = ifelse(FamilyHistoryAlzheimers == 0, "No", "Yes")) %>%
  mutate(CardiovascularDisease = ifelse(CardiovascularDisease == 0, "No", "Yes")) %>%
  mutate(Diabetes = ifelse(Diabetes == 0, "No", "Yes")) %>%
  mutate(Depression = ifelse(Depression == 0, "No", "Yes")) %>%
  mutate(HeadInjury = ifelse(HeadInjury == 0, "No", "Yes")) %>%
  mutate(Hypertension = ifelse(Hypertension == 0, "No", "Yes")) %>%
  mutate(MemoryComplaints = ifelse(MemoryComplaints == 0, "No", "Yes")) %>%
  mutate(BehavioralProblems = ifelse(BehavioralProblems == 0, "No", "Yes")) %>%
  mutate(Confusion = ifelse(Confusion == 0, "No", "Yes")) %>%
  mutate(Disorientation = ifelse(Disorientation == 0, "No", "Yes")) %>%
  mutate(PersonalityChanges = ifelse(PersonalityChanges == 0, "No", "Yes")) %>%
  mutate(DifficultyCompletingTasks = ifelse(DifficultyCompletingTasks == 0, "No", "Yes")) %>%
  mutate(Forgetfulness = ifelse(Forgetfulness == 0, "No", "Yes")) %>%
  mutate(Diagnosis = ifelse(Diagnosis == 0, "Negative", "Positive"))
  
```

## Summarizing and Visualizing
```{r}
# correlation matrix of all potential predictors and response variable MMSE
alz_cat_data2 = alz_cat_data[,c('MMSE','Age','BMI','AlcoholConsumption','PhysicalActivity',
                                 'DietQuality','SleepQuality','SystolicBP','DiastolicBP',
                                 'CholesterolTotal','CholesterolLDL','CholesterolHDL',
                                 'CholesterolTriglycerides','FunctionalAssessment',
                                 'ADL')]

cor_mat = round(cor(alz_cat_data2),2)
ggcorrplot(cor_mat, lab=TRUE, lab_size=2, tl.cex = 10, method = "square")+
  scale_fill_gradient2(limits = c(-0.2, 0.2), low = "blue", mid = "white", high = "red", midpoint = 0, space = "Lab", name = "Correlation")
```
```{r}
# Scatter plots of numerical predictors vs response
long_format_data = gather(alz_cat_data, key="predictor", value ="value", 
                   Age, BMI, AlcoholConsumption, PhysicalActivity, DietQuality, SleepQuality, SystolicBP, 
                   DiastolicBP, CholesterolTotal, CholesterolLDL, CholesterolHDL, CholesterolTriglycerides,
                   FunctionalAssessment,ADL)

ggplot(long_format_data, aes(x=value, y=MMSE, color=predictor)) +
  geom_point(size=0.01)+
  facet_wrap(~predictor, scales = "free_x", ncol=4) #frees x axis to look at range of all distinct predictors
```

# Building MLR model

```{r}
# MLR model with all possible numerical predictors
model = lm(MMSE~Age+BMI+AlcoholConsumption+PhysicalActivity+DietQuality+SleepQuality+SystolicBP+
             DiastolicBP+CholesterolTotal+CholesterolLDL+CholesterolHDL+CholesterolTriglycerides+
             FunctionalAssessment+ADL+
             Gender+Ethnicity+EducationLevel+Smoking+FamilyHistoryAlzheimers+CardiovascularDisease+
             Diabetes+Depression+HeadInjury+Hypertension+MemoryComplaints+BehavioralProblems+
             Confusion+Disorientation+PersonalityChanges+DifficultyCompletingTasks+Forgetfulness, data=alz_cat_data)
summary(model)
```

```{r}
# Variable selection
aic = stepAIC(model, direction='both', trace=F)
summary(aic)
```
```{r}
# Preform log transformations
alz_log_data = mutate(alz_cat_data, log_Age = log(Age), 
                                    log_BMI = log(BMI),
                                    log_AlcoholConsumption = log(AlcoholConsumption),
                      log_PhysicalActivity = log(PhysicalActivity),
                      log_DietQuality = log(DietQuality),
                      log_SleepQuality = log(SleepQuality),
                      log_SystolicBP = log(SystolicBP),
                      log_DiastolicBP = log(DiastolicBP),
                      log_CholesterolTotal = log(CholesterolTotal),
                      log_CholesterolLDL = log(CholesterolLDL),
                      log_CholesterolHDL = log(CholesterolHDL),
                      log_CholesterolTriglycerides = log (CholesterolTriglycerides),
                      log_FunctionalAssessment = log(FunctionalAssessment),
                      log_ADL = log(ADL))

alz_log_data = subset(alz_log_data, select = -c(Age,BMI,AlcoholConsumption,PhysicalActivity,
                                               DietQuality,SleepQuality,SystolicBP,DiastolicBP,
                                               CholesterolTotal, CholesterolLDL,CholesterolHDL,
                                               CholesterolTriglycerides,FunctionalAssessment,ADL))

# keep response, log transform all predictors
model_log = lm(MMSE~log_Age+log_BMI+log_AlcoholConsumption+log_PhysicalActivity+log_DietQuality+log_SleepQuality+log_SystolicBP+
             log_DiastolicBP+log_CholesterolTotal+log_CholesterolLDL+log_CholesterolHDL+log_CholesterolTriglycerides+
             log_FunctionalAssessment+log_ADL+
             Gender+Ethnicity+EducationLevel+Smoking+FamilyHistoryAlzheimers+CardiovascularDisease+
             Diabetes+Depression+HeadInjury+Hypertension+MemoryComplaints+BehavioralProblems+
             Confusion+Disorientation+PersonalityChanges+DifficultyCompletingTasks+Forgetfulness, data=alz_log_data)
summary(model_log)
```
```{r}
# Residual plot with all log transformed and categorical predictors 
alz_log_data_fitted = mutate(alz_log_data, pred=fitted(model_log), resid=residuals(model_log))

ggplot(alz_log_data_fitted, aes(x=pred, y=resid))+
  geom_point()+
  geom_hline(yintercept =0, color="red")
```
```{r}
# QQ plot with all log transformed and categorical predictors
ggplot(alz_log_data_fitted, aes(sample=resid)) +
  stat_qq() +
  stat_qq_line(color="red")
```

SELECTING THE MOST SIGNIFICANT VARIABLES WITH AIC:
```{r, trace=0}
# Using AIC for variable selection for log transformed model
aic = stepAIC(model_log, direction='both', trace=F)
summary(aic)
```
```{r}
# finding VIFs values for AIC identified predictors
vif(aic)
```
```{r}
# model with three variables identified by AIC
selected_mod = lm(MMSE~log_DiastolicBP+Gender+Disorientation, data=alz_log_data)
summary(selected_mod)
```
```{r}
# residuals for AIC log transformed model:
alz_log_data_selected_fitted = mutate(alz_log_data, pred=fitted(selected_mod), resid=residuals(selected_mod))
ggplot(alz_log_data_selected_fitted , aes(x=pred, y=resid))+
  geom_point()+
  geom_hline(yintercept =0, color="red")
```
```{r}
# QQ plot: checking normality assumption, looking at distribution of residuals
ggplot(alz_log_data_selected_fitted , aes(sample=resid)) +
  stat_qq() +
  stat_qq_line(color="red")
```
LOOKING FOR OUTLIERS, LEVERGE POINTS, INFLUENTIAL POINTS
```{r}
diagnostics = selected_mod%>%
  augment(data = alz_log_data)

outliers = filter(diagnostics, abs(.std.resid)>3)
outliers # no outliers!
```
```{r}
leverage = filter(diagnostics, .hat>2*(3+1)/nrow(alz_log_data))
nrow(leverage) # 163 influential points
```
```{r}
influence = filter(diagnostics, .cooksd>4/nrow(alz_log_data))
nrow(influence) # 64 influential points
```


```{r}
# investigating overlap between leverage points and influential points

common_values <- inner_join(influence, leverage, by = "PatientID")
common_values%>%
  dplyr::select(PatientID, .hat.x, .cooksd.x) 

# 29 observations both influential and leverage points, trim these
```
```{r}
# trimming common values from log transformed data
alz_log_data_trimmed = anti_join(alz_log_data, common_values, by = "PatientID")
```

```{r}
# Re-try model!
model_log_trimmed = lm(MMSE~log_Age+log_BMI+log_AlcoholConsumption+log_PhysicalActivity+log_DietQuality+log_SleepQuality+log_SystolicBP+
             log_DiastolicBP+log_CholesterolTotal+log_CholesterolLDL+log_CholesterolHDL+log_CholesterolTriglycerides+
             log_FunctionalAssessment+log_ADL+
             Gender+Ethnicity+EducationLevel+Smoking+FamilyHistoryAlzheimers+CardiovascularDisease+
             Diabetes+Depression+HeadInjury+Hypertension+MemoryComplaints+BehavioralProblems+
             Confusion+Disorientation+PersonalityChanges+DifficultyCompletingTasks+Forgetfulness, data=alz_log_data_trimmed)

aic = stepAIC(model_log_trimmed, direction='both', trace=F)
summary(aic)
```

```{r}
# trimmed model with three variables identified by AIC
trimmed_selected_mod = lm(MMSE~log_DiastolicBP+PersonalityChanges+Disorientation, data=alz_log_data_trimmed)
summary(trimmed_selected_mod)
```
```{r}
# summarizing models 
summary_table <- data.frame(
  Model = c('all numerical and categorical predictors',
            'log transformations',
            'AIC selected predictors from transformed model',
            'AIC selected predictors from transformed model trimmed for influential/leverage points'
            ),
  PValue = c(0.97, 0.9698, 0.0992, 0.03919),
  AdjustedR2 = c(-0.006573, -0.006564, 0.001523, 0.002528),
  ResidSE = c(8.641,8.641,8.607,8.533)
)
summary_table
```


CROSS VALIDATION FOR BEST MODEL
```{r}
# use caret package to build and cross validate model

# cross validate
control = trainControl(method = 'repeatedcv', number=5, repeats=10) #more repetitions introduce more variability

# create/train the model with cv built in
trimmed_selected_mod2 = train(MMSE~log_DiastolicBP+PersonalityChanges+Disorientation, method='lm', trControl=control, data=alz_log_data_trimmed)

# checking root mean square error, R2, MAE
trimmed_selected_mod2$results$RMSE
trimmed_selected_mod2$results$Rsquared
trimmed_selected_mod2$results$MAE
```

LASSO REGULARIZATION:
```{r}
# take out diagnosis
alz_cat_data3 = subset(alz_cat_data, select = -c(Diagnosis))

X = model.matrix(MMSE~0+.,data=alz_cat_data3) # the ~0 gets rid of all 1s in order to use glmnet
y = alz_cat_data3$MMSE # response variable

rmodel = glmnet(x=X, y=y, alpha = 1)

kcvglmnet = cv.glmnet(x=X, y=y, alpha =1, nfolds=5)

predict(rmodel, type="coefficients", s=kcvglmnet$lambda.min, newx = X[1:2,])
```

```{r}
plot(rmodel, label=T, xvar="lambda")+abline(v=log(kcvglmnet$lambda.min))
```
```{r}
# variables that shrunk to zero last:
# 10: EducationLevelNone 
# 34 DisorientationYes 
# 35 PersonalityChangesYes 

trimmed_selected_mod3 = lm(MMSE~EducationLevel+PersonalityChanges+Disorientation, data=alz_log_data_trimmed)
summary(trimmed_selected_mod3)
# does not preform better than previous model
```

MAKING A PREDICTION WITH THE BEST MODEL:
```{r}
new_dat = data.frame(log_DiastolicBP=72,PersonalityChanges='No',Disorientation='No')
# predicted
predict(trimmed_selected_mod, new_dat)
```
```{r}
# actual
alz_log_data_trimmed[1,12]
```

```{r}
# prediction interval
predict(trimmed_selected_mod, newdata = new_dat, interval = "prediction", level=0.95)
```
We are 95% confident that a patient with log_DiastolicBP=72,PersonalityChanges='No',Disorientation='No' 
has an MMSE score between -211.6598 and 34.95187.



